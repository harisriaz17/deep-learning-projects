{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"poetry_generator.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"zMkw96ZTKbFi","colab_type":"text"},"cell_type":"markdown","source":["# POETRY GENERATION USING LSTM NETWORKS (CHARACTER-LEVEL MODEL)"]},{"metadata":{"id":"Kr-6JffyNzdn","colab_type":"text"},"cell_type":"markdown","source":["###POETRY GENERATION IS A TEXT GENERATION PROBLEM THAT DEALS WITH SEQUENTIAL DATA.\n","###Sequential data can be defined as folows:\n","\n","\n","\n","###Any data which has two events occurring in a particular time frame and the occurrence of event A before event B is an entirely different scenario as compared to the occurrence of event A after event B.\n","\n","###In traditional ML problems, the order of occurrence of data points is not important. However in sequence prediction problems such as character-by-character text generation, our model has to output the correct character every time otherwise whole sequences of characters preceding it may become meaningless.\n","\n","\n","###This is what makes text/poetry generators tricky and to solve this problem we will use LSTM (Long Short Term Memory) networks which are a special kind of RNN's. LSTM's are well-suited to the task of capturing and remembering long-term dependencies. They are excellent at capturing context in sequential data and remembering it for long periods of time."]},{"metadata":{"id":"mscbw1nk1wS3","colab_type":"text"},"cell_type":"markdown","source":["###In this notebook, we will train an LSTM using Keras on a collection of sonnets by Robert Frost (the famous American poet) which includes works like:\n","###*The Road Not Taken*\n","###*Mending Wall*\n","###*Nothing Gold Can Stay* \n","###among others.\n","![alt text](https://revbrocoach.files.wordpress.com/2014/03/two-roads-robert-frost.jpg)"]},{"metadata":{"id":"ApU1fkHkSDAl","colab_type":"code","outputId":"27800e92-605b-4211-8ee9-61e5a28c9c32","executionInfo":{"status":"ok","timestamp":1548436933482,"user_tz":-300,"elapsed":1418,"user":{"displayName":"Haris Riaz","photoUrl":"https://lh6.googleusercontent.com/-uYlieiRYDZk/AAAAAAAAAAI/AAAAAAAAADg/FSnrBXtYgYE/s64/photo.jpg","userId":"02091847136436119698"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/\n","%cd cs231n/\n","%cd poetry_generator"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive\n","/content/drive/My Drive/cs231n\n","/content/drive/My Drive/cs231n/poetry_generator\n"],"name":"stdout"}]},{"metadata":{"id":"mVqL2zbp3eLj","colab_type":"text"},"cell_type":"markdown","source":["##IMPORTING LIBRARIES"]},{"metadata":{"id":"IYcmL7vjTkb7","colab_type":"code","outputId":"2ed5841b-7896-41f6-ec4e-f258b8e2e71c","executionInfo":{"status":"ok","timestamp":1548481802886,"user_tz":-300,"elapsed":1955,"user":{"displayName":"Haris Riaz","photoUrl":"https://lh6.googleusercontent.com/-uYlieiRYDZk/AAAAAAAAAAI/AAAAAAAAADg/FSnrBXtYgYE/s64/photo.jpg","userId":"02091847136436119698"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import RNN\n","from keras.utils import np_utils"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"g5ohtLKd-TY-","colab_type":"text"},"cell_type":"markdown","source":["##LOADING DATA"]},{"metadata":{"id":"pf8te9cuTzDx","colab_type":"code","colab":{}},"cell_type":"code","source":["text = (open(\"robert_frost.txt\").read())\n","text = text.lower()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6SMWKDNGniau","colab_type":"text"},"cell_type":"markdown","source":["##MAPPING CHARACTERS TO NUMBERS"]},{"metadata":{"id":"GazOMt-OT9zl","colab_type":"code","colab":{}},"cell_type":"code","source":["# since machines understand numbers better than words or characters, here we will map unique character to an arbitrary unique number to speed up training\n","characters = sorted(list(set(text)))\n","\n","n_to_char = {n:char for n, char in enumerate(characters)}\n","char_to_n = {char:n for n, char in enumerate(characters)}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GYvNrSCIoEFd","colab_type":"text"},"cell_type":"markdown","source":["## PREPROCESSING OUR DATA"]},{"metadata":{"id":"sF8hqmLtUgBL","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","X = [] # train array\n","Y = [] # target array\n","length = len(text) # total number of characters in text\n","seq_length = 100 # length of sequence of characters to consider before outputting next char in sequence\n","\n","# creating sequences of 100 chars each stored in sequence variable\n","# label stores the next/correct prediction of each sequence or the 101st value in every sequence \n","for i in range(0, length-seq_length, 1):\n","    sequence = text[i:i + seq_length]\n","    label =text[i + seq_length]\n","    X.append([char_to_n[char] for char in sequence])\n","    Y.append(char_to_n[label])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eEHbTYL-Vb_z","colab_type":"code","colab":{}},"cell_type":"code","source":["# LSTM requires training data to be of shape (number_of_sequences, length_of_sequence, number_of_features) hence reshape X\n","X_modified = np.reshape(X, (len(X), seq_length, 1))\n","# scale X for faster training\n","X_modified = X_modified / float(len(characters))\n","# one-hot encode Y to prevent any logical issues arising from mapping characters to numbers\n","Y_modified = np_utils.to_categorical(Y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bw0a9k_vpQUH","colab_type":"text"},"cell_type":"markdown","source":["##DEFINING LSTM MODEL"]},{"metadata":{"id":"KllHFlM7VRkI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Network has 3 layers of 700 neurons each with Dropout set to 20%\n","model = Sequential()\n","model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(700, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(700))\n","model.add(Dropout(0.2))\n","model.add(Dense(Y_modified.shape[1], activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NlY9CFYnp2_M","colab_type":"text"},"cell_type":"markdown","source":["## TRAIN MODEL"]},{"metadata":{"id":"eX-eY4mqVW1O","colab_type":"code","colab":{}},"cell_type":"code","source":["# training code -- use pre-trained weights instead as this may take upwards of 12 hours on a Tesla K80 equivalent GPU when epoch = 100\n","#model.fit(X_modified, Y_modified, epochs=100, batch_size=100)\n","#model.save_weights('frostweights.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"krfjBbqpqBmo","colab_type":"text"},"cell_type":"markdown","source":["##LOAD PRE-TRAINED WEIGHTS"]},{"metadata":{"id":"i9R1K_MUWHbk","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_weights('weights.h5')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Q2HpETqqGVj","colab_type":"text"},"cell_type":"markdown","source":["##GENERATE TEXT"]},{"metadata":{"id":"mMBOxsqcXI-O","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# select random row from X which is sequence of 100 characters\n","# predict next 100 characters following this row\n","string_mapped = X[99]\n","full_string = [n_to_char[value] for value in string_mapped]\n","# generating total 2000 characters\n","for i in range(2000):\n","    #reshape input to previous shape and rescale to original scale\n","    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n","    x = x / float(len(characters))\n","\n","    # select character (number) with max probability\n","    pred_index = np.argmax(model.predict(x, verbose=0))\n","    # convert integers back to characters\n","    seq = [n_to_char[value] for value in string_mapped]\n","    \n","    # remove first character from 100 character string array and append newly predicted character\n","    full_string.append(n_to_char[pred_index])\n","    string_mapped.append(pred_index)\n","    string_mapped = string_mapped[1:len(string_mapped)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oyWZPkE_ruMJ","colab_type":"text"},"cell_type":"markdown","source":["## PRINT GENERATED RHYMES (2000 CHARACTERS)"]},{"metadata":{"id":"BNTn8poTdk4q","colab_type":"code","outputId":"f7e710e3-2104-4ab3-b510-b416d6c0e5d4","executionInfo":{"status":"ok","timestamp":1548441077698,"user_tz":-300,"elapsed":457378,"user":{"displayName":"Haris Riaz","photoUrl":"https://lh6.googleusercontent.com/-uYlieiRYDZk/AAAAAAAAAAI/AAAAAAAAADg/FSnrBXtYgYE/s64/photo.jpg","userId":"02091847136436119698"}},"colab":{"base_uri":"https://localhost:8080/","height":1004}},"cell_type":"code","source":["#combining text\n","txt=\"\"\n","for char in full_string:\n","  txt = txt+char\n","  \n","#print generated text\n","print(txt)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["g i stood\n","and looked down one as far as i could\n","to where it bent in the undergrowth; \n","\n","then took the one less traveles had seen he chen't recognize,\n","but welcomed for the place. \n","this he sat listening to till she tound of feet\n","when far away an interrupted cry\n","came over houses from another street,\n","\n","but not to call me back or say good-bye on meet ald stam fected to with sawdust\n","and swollen tight and buried under snow.\n","the nean see how the house as far as i could\n","the woods and frozen lake\n","the dourap and hels of cole\n","that soulde\n","to each other \n","till well toward nownt have up hope\n","of getting home again because\n","he couldn't climb that slippery slope\n","(two miles it was) to his abode.\n","\n","sometimes as an authority\n","on motor-cars, i'm asked if i\n","should say our stock was petered out of here\n","to say that for eestruction ice\n","is also great\n","and would suffice.\n","\n","nt mother woold ho befn the stairs, the swo stiel of them all aglitter,\n","and birds that joined in the excited fun\n","by doubling and redoubling song and twitter,\n","i have no doubt i'd end by holding none.\n","\n","it takes the moon for this. the sun's a wizard\n","by all i tell; but so's the moon a witch.\n","from the high west she makes a gentle cast\n","and suddenly, without a jerk or twitchng on a scrien: \n","the meaning of the manter oatsrali?\n","\n","the young folk held some hope out to each other \n","till well toward noon when the storm settled down \n","with a swish in the grass. 'what if the others \n","are there,' they said. 'it isn't going to rain.' \n","'it's raining.' \n","'no, it's misting; let's be fair. \n","does the rain seem to you to cool the eyes?' \n","the situation was like this: the road \n","bowed outward on the mountain half-way up, \n","and disappeared and ended not far off. \n","no one went home that way. the only house \n","beyond where they were was a shattered seedpod. \n","and below roared a brook hidden in trees, \n","the sound of which was silence for the place. \n","this he sat listening to till she gave judgment. \n","'on father's side, it seems, we're- let me see- - ' \n","'don't be too'technical.- you have three cards.' \n","'four cards, one yours, three mine, one for each branch \n","of the stark fa\n"],"name":"stdout"}]},{"metadata":{"id":"HZXRew2nrzER","colab_type":"text"},"cell_type":"markdown","source":["###AS WE CAN SEE, THE ALGORITHM DID START GRASPING THE STRUCTURE OF ROBERT FROST'S POETRY AND MANAGED TO PRODUCE A SOMEWHAT DECENT/COHERENT COLECTION OF RHYMES. \n","###THE REAL PROBLEM WAS THAT WE WERE GPU LIMITED TO ONLY 30 EPOCH'S OVER THE TRAINING SET. THIS IS WHY WE SEE SEVERAL SPELLING MISTAKES IN THE GENERATED TEXT. THE MODEL WAS TRAINED ON A TESLA K80 GPU INSIDE GOOGLE COLAB AND I DID NOT HAVE ENOUGH TIME TO TRAIN IT FOR THE FULL 100 EPOCH'S OVER THE TRAINING DATA.\n","###A MORE POWERFUL GPU WITH MORE TRAINING HOURS WILL RESULT IN VERY ACCURATE, BEAUTIFUL RHYMES."]}]}